# -*- coding: utf-8 -*-
"""Airline Review Intelligence: Predicting Satisfaction and Service Insights

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLbBoHcHP8QBUHhE-9LC3NTDE6prI8KA

# **Project Name**    - "Airline Review Intelligence: Predicting Satisfaction and Service Insights"

##### **Project Type**    - EDA/Classification
##### **Contribution**    - Individual

# **Project Summary -**

Write the summary here within 500-600 words.

The airline review dataset provides insights into customer experiences across various airlines, allowing us to investigate patterns and factors that influence customer satisfaction. The dataset includes 17 columns with information on key aspects such as overall ratings, seat comfort, cabin service, food and beverage quality, entertainment, ground service, and value for money. Each review is associated with details about the reviewer, the travel date, type of aircraft, cabin class, traveler type (e.g., business or leisure), route, and whether the reviewer recommends the airline. This project aims to leverage the data to uncover trends, correlations, and drivers of positive and negative experiences, helping airlines make data-driven improvements to customer service.

Objectives
Identify Key Drivers of Customer Satisfaction: Using correlations and detailed analysis, we aim to identify which factors (such as seat comfort, entertainment, or food quality) contribute most significantly to overall customer satisfaction. By understanding these drivers, airlines can prioritize improvements in areas that most impact customer perceptions.

Analyze Trends Over Time: By exploring the review_date column, we will analyze how customer satisfaction has evolved over time. This temporal analysis may reveal trends in service quality, the impact of external factors (e.g., changes due to COVID-19), or how airlines have adapted their services over the years.

Evaluate Airline Performance: By comparing the average ratings across airlines, we can rank them according to customer satisfaction. This will reveal the best and worst-performing airlines, highlighting industry leaders and areas for improvement among competitors.

Understand Traveler Preferences: By examining traveler types (such as solo, family, or business travelers) and cabin classes (economy, business, or first class), we aim to understand how experiences vary by traveler segment. This analysis can inform targeted service enhancements, allowing airlines to cater more effectively to different customer needs.

Data Analysis Techniques
Exploratory Data Analysis (EDA): We will start with EDA to examine distributions, detect outliers, and understand basic statistics related to customer ratings and reviews. This includes visualizing rating distributions, top routes, and popular airlines through histograms, bar charts, and pie charts.

Correlation Analysis: A correlation heatmap will be used to explore relationships between the various ratings (e.g., seat comfort, entertainment, cabin service, etc.). This will help us identify which service aspects are most strongly correlated with overall satisfaction.

Sentiment Analysis (Optional): If time permits, we may analyze the customer_review text column for sentiment, categorizing reviews as positive, neutral, or negative. This qualitative analysis would provide insights into common themes and recurring customer concerns, complementing the quantitative rating data.

Time Series Analysis: With review_date and date_flown columns, we will analyze changes in ratings over time. This will allow us to see if certain airlines or services have improved or declined in customer satisfaction.

Visualization Techniques
Using libraries like seaborn and matplotlib, visualizations such as bar charts, box plots, pie charts, and heatmaps will convey findings effectively. For instance, a heatmap will show correlations among ratings, while box plots will compare satisfaction across cabin types.

Expected Outcomes
This analysis will provide insights into the strengths and weaknesses of different airlines, helping them to improve customer experience. For instance, if seat comfort shows a strong correlation with overall satisfaction, airlines might focus on upgrading seating across cabins. Similarly, understanding traveler-type preferences allows airlines to personalize services based on customer profiles.

By presenting these insights in a concise visual dashboard, the project can assist airlines in enhancing service quality, thereby improving their market position and customer loyalty.

# **GitHub Link -**

Provide your GitHub Link here.
https://github.com/Vivek2292kumar/-Project-Name---Airline-Review-Intelligence-/upload/main

# **Problem Statement**

**Write Problem Statement Here.**

The airline industry is highly competitive, with customer satisfaction being a critical factor influencing customer loyalty and brand reputation. This dataset includes various customer-related attributes from airline reviews, such as overall ratings, comfort and service factors, and travel details. The goal of this project is twofold:

Customer Sentiment Analysis and Satisfaction Insights: Analyze customer reviews and ratings to understand what factors most significantly influence overall customer satisfaction. Identify patterns that show which service features (e.g., cabin comfort, in-flight entertainment, and food quality) impact the likelihood of a customer recommending the airline.

Predictive Modeling for Recommendation: Develop a machine learning model that predicts whether a customer would recommend the airline based on the provided features. By understanding which factors drive recommendations, the airline can optimize its services to improve customer loyalty and satisfaction.

Through data analysis and machine learning, this project aims to generate actionable insights into customer preferences and develop a predictive model to help airlines focus on service aspects that enhance customer satisfaction and brand loyalty.

# **General Guidelines** : -

1.   Well-structured, formatted, and commented code is required.
2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.
     
     The additional credits will have advantages over other students during Star Student selection.
       
             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go
                       without a single error logged. ]

3.   Each and every logic should have proper comments.
4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.
        

```
# Chart visualization code
```
            

*   Why did you pick the specific chart?
*   What is/are the insight(s) found from the chart?
* Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

5. You have to create at least 15 logical & meaningful charts having important insights.


[ Hints : - Do the Vizualization in  a structured way while following "UBM" Rule.

U - Univariate Analysis,

B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)

M - Multivariate Analysis
 ]





6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.


*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.


*   Cross- Validation & Hyperparameter Tuning

*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

# ***Let's Begin !***

## ***1. Know Your Data***

### Import Libraries
"""

# Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import scipy.stats as stats
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from scipy import stats
import re
import nltk
from nltk import pos_tag

import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import pos_tag

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

"""### Dataset Loading"""

# Load Dataset
from google.colab import drive
drive.mount('/content/drive')

"""### Dataset First View"""

# Dataset First Look
os.chdir('/content/drive/MyDrive/module 6')
df = pd.read_excel('data_airline_reviews.xlsx')
df.head()

"""### Dataset Rows & Columns count"""

# Dataset Rows & Columns count
df.shape

"""### Dataset Information"""

# Dataset Info
df.info()

"""#### Duplicate Values"""

# Dataset Duplicate Value Count
df.duplicated().sum()

"""#### Missing Values/Null Values"""

# Missing Values/Null Values Count
df.isnull().sum()

# Visualizing the missing values
sns.heatmap(df.isnull() , cbar = False)

"""### What did you know about your dataset?

Answer Here
From the given dataset we can see dataset having the shape of (131895, 17)
where data contains 131895 rows and 17 columns ...
some columns  of the datasemt needs change on datatype
like "review_date" , "date_flown" need change data type
Dataset also having some missing values to impute

## ***2. Understanding Your Variables***
"""

# Dataset Column
df.columns

# Dataset Describe
df.describe()

"""### Variables Description

Answer Here

**airline:** The name of the airline

**overall:** The overall rating given by the reviewer

**author:** The name or username of the reviewer.

**review_date:** The date when the review was written.


**customer_review:**  The detailed text review provided by the customer, describing their experience.

**aircraft:** The type or model of aircraft used for the flight.

**traveller_type:**The type of traveler, such as Business, Leisure, Solo, or Family

**cabin:** The class in which the customer traveled, such as Economy, Business, or First Class.

**route:** The flight route, generally represented as "departure airport - arrival airport."

**date_flown:** The actual date the customer flew on the airline.

**seat_comfort:** Rating for seat comfort

**cabin_service:** Rating for the service quality in the cabin

**food_bev:**  Rating for the food and beverage quality provided during the flight.

**entertainment:** Rating for in-flight entertainment options.

**ground_service:** Rating for ground service, such as check-in and boarding assistance.

**value_for_money:** Rating indicating if the customer felt they received good value for the money spent.

**recommended:** Indicates if the customer recommends the airline, usually a Yes/No response.

### Check Unique Values for each variable.
"""

# Check Unique Values for each variable.
df.nunique()

"""## 3. ***Data Wrangling***

### Data Wrangling Code
"""

# Write your code to make your dataset analysis ready.
# chage data type of columns need
df['review_date'] = pd.to_datetime(df['review_date'] , errors = 'coerce')
df['date_flown'] = pd.to_datetime(df['date_flown'])

# impute missing values
# check persent of missig values in all columns
persent = df.isnull().mean()*100
print(persent)

# impute missing values for categorical columns
# first check for missing  values
categorical_columns = []
for i in df.columns:
  if df[i].dtype == 'object':
    categorical_columns.append(i)
categorical_columns
for col in  categorical_columns:
  df[col].fillna(df[col].mode()[0], inplace = True)
categorical_columns

# find numeric_data_type columns and impute missing values
numeric_columns =  []
for col in df.columns:
  if df[col].dtype == 'int64' or df[col].dtype == 'float64':
   numeric_columns.append(col)
numeric_columns
for col in numeric_columns:
  df[col].fillna(df[col].mean(), inplace = True)

# impute missing values of datetime columns
df['review_date'].fillna(df['review_date'].mode()[0] , inplace = True)
df['date_flown'].fillna(df['date_flown'].mode()[0] , inplace = True)

# check persen of duplicated values  in the dataset
df.duplicated().mean()*100
# remove duplicate value
df = df.drop_duplicates()

df.head(5)

"""### What all manipulations have you done and insights you found?"""



"""Answer Here.

1 - Change data type of required columns

2 - check for missing value perset for all columns

3 - Impute missing values by mean , mode for numerical columns ,  categorical columns and ffill , bfill for datetime columns

4 -  Check duplicated values

5 - remove duplicated values

## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***

#### Chart - 1
"""

# Chart - 1 visualization code
# show top airlines by average rating
top_airlines = df.groupby('airline')['overall'].mean().sort_values(ascending=False).head(10)
sns.barplot(x=top_airlines.values, y=top_airlines.index)
plt.xlabel('Average Rating')
plt.ylabel('Airline')
plt.title('Top 10 Airlines by Average Rating')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.  I use this chat to  clearly ranks airlines based on customer satisfaction, making it easy to identify leaders and laggards.

##### 2. What is/are the insight(s) found from the chart?

Answer Here  with the help of this chart we can see top 10 airlines by average rating

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here
**Positive Business Impact:** Airlines can leverage this insight to promote their high ratings, enhancing their brand image and attracting new customers.

**Negative Growth Insight:** If an airline ranks low, it may indicate poor service or operational issues, necessitating urgent action to avoid losing market share.

#### Chart - 2
"""

# Chart - 2 visualization code
# Shows how ratings are distributed across all reviews.
plt.hist(df['overall'], bins=10, color='skyblue', edgecolor='black')
plt.xlabel('Overall Rating')
plt.ylabel('Frequency')
plt.title('Distribution of Overall Ratings')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.   With the help of this chart we can see the frequency distribution of over all rating

##### 2. What is/are the insight(s) found from the chart?

Answer Here.  we the help of this chart we can see high rating is from 0 to 2  , medium for 8 tp 10 and less for 4 to 6

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

**Positive Business Impact:** Understanding the distribution helps airlines identify areas for improvement, especially if a significant number of ratings are clustered at the lower end.

**Negative Growth Insight:** A left-skewed distribution may indicate systemic issues in service, suggesting that focused improvements are required to elevate the overall customer experience.
3. Pie Chart: Traveller Types

#### Chart - 3
"""

# Chart - 3 visualization code
# Visualizes the proportion of different traveler types
traveller_counts = df['traveller_type'].value_counts()
plt.pie(traveller_counts, labels=traveller_counts.index, autopct='%1.1f%%')
plt.title('Traveller Type Distribution')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here. we use this chart to  showcases the composition of traveler demographics, providing insights into customer segments.

##### 2. What is/are the insight(s) found from the chart?

Answer Here
From the chart we can see 61% from all was booked by Solo Leisure traveller type and  and remaining distributed amoung three Business (11.1%) , couple leisure (15.9 %) , Family Leisure(11.7%)

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

**Positive Business Impact:** Tailoring services to meet the specific needs of dominant traveler types (e.g., business vs. leisure) can enhance customer satisfaction and retention.

**Negative Growth Insight:** If certain traveler segments are underrepresented, it may highlight a missed market opportunity that could hinder growth.

#### Chart - 4
"""

# Chart - 4 visualization code
# Seat Comfort by Cabin Type
sns.boxplot(x='cabin', y='seat_comfort', data=df)
plt.xlabel('Cabin Type')
plt.ylabel('Seat Comfort Rating')
plt.title('Seat Comfort by Cabin Type')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.  I use this cahrt to compares ratings across different cabin types, indicating variability in customer experiences.

##### 2. What is/are the insight(s) found from the chart?

Answer Here From the chart we see seat comfort rating are higher for Business class and First class  and less for Economy and Preminum Class

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

**Positive Business Impact:** Identifying cabins with lower comfort ratings allows airlines to make targeted upgrades, potentially boosting overall satisfaction and loyalty.



**Negative Growth Insight:** A significant difference in ratings may expose service quality discrepancies that can drive customers away, especially in premium cabins.

#### Chart - 5
"""

# Chart - 5 visualization code
# Average Rating Trend Over Time
rating_trend = df.groupby(df['review_date'].dt.to_period('M'))['overall'].mean()
rating_trend.plot(kind='line')
plt.xlabel('Review Date')
plt.ylabel('Average Rating')
plt.title('Trend of Average Rating Over Time')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.

The line chart helps visualize how ratings change over time, indicating trends.

##### 2. What is/are the insight(s) found from the chart?

Answer Here
From the line chart we can see rating was constant at 5 from year 2003 to 2008
after 2008 it was went  down but but after 2009 to 2015 it was changing but was higher after 2015 it went down.

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

**Positive Business Impact:** Airlines can assess the effectiveness of changes made to services or policies and adapt strategies accordingly.

**Negative Growth Insight:** A downward trend in ratings could signify deteriorating service quality or customer dissatisfaction over time, which needs immediate attention.

#### Chart - 6
"""

# Chart - 6 visualization code
# Correlation Between Ratings
rating_columns = ['overall', 'seat_comfort', 'cabin_service', 'food_bev', 'entertainment', 'ground_service', 'value_for_money']
sns.heatmap(df[rating_columns].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Rating Attributes')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here. With the help of heatmap we can  identifies relationships between various aspects of the service.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

 From the chart we can see seat comfort and overall are highely co-related , cabin_service and seat comfort are hid=ghely co-related ,

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here  
**Positive Business Impact:** Insights can prioritize areas for improvement that significantly influence overall satisfaction, enabling strategic investments.

**Negative Growth Insight:** Weak or negative correlations between crucial service attributes may indicate systemic issues requiring extensive review and overhaul.

#### Chart - 7
"""

# Chart - 7 visualization code
# Route Popularity
top_routes = df['route'].value_counts().head(10)
sns.barplot(x=top_routes.values, y=top_routes.index)
plt.xlabel('Frequency')
plt.ylabel('Route')
plt.title('Top 10 Routes by Popularity')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here. I pick this chart to highlights the most traveled routes, which are crucial for operational focus.

##### 2. What is/are the insight(s) found from the chart?

Answer Here  From the chart we can see the route was used most is Bangkok to Hongkong

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here  

**Positive Business Impact:** Airlines can enhance services on popular routes to maximize customer satisfaction and increase profitability.

**Negative Growth Insight:** Underperforming routes may indicate issues with service quality, pricing, or competition, warranting reassessment.

#### Chart - 8
"""

# Chart - 8 visualization code
# Recommended vs. Not Recommended
sns.countplot(x='recommended', data=df)
plt.xlabel('Recommended')
plt.ylabel('Count')
plt.title('Recommendation Count')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.  
I pick this chart summarizes customer recommendations, a strong indicator of satisfaction.

##### 2. What is/are the insight(s) found from the chart?

Answer Here   From the chart we can see Recommended are less then not recomended

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here  

**Positive Business Impact:** High recommendation rates can be used in marketing efforts to attract new customers.

**Negative Growth Insight:** A significant proportion of non-recommendations may highlight critical service failures or customer grievances that need addressing.

#### Chart - 9
"""

# Chart - 9 visualization code
# Value for Money vs. Overall Rating
plt.scatter(df['overall'], df['value_for_money'], alpha=0.5)
plt.xlabel('Overall Rating')
plt.ylabel('Value for Money')
plt.title('Value for Money vs. Overall Rating')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.  I pick this specific chart to visualizes the relationship between perceived value and satisfaction.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

#### Chart - 10
"""

# Chart - 10 visualization code
# Ground Service Ratings by Airline

top_airlines = df['airline'].value_counts().index[:10]
sns.boxplot(x='airline', y='ground_service', data=df[df['airline'].isin(top_airlines)])
plt.xticks(rotation=90)
plt.xlabel('Airline')
plt.ylabel('Ground Service Rating')
plt.title('Ground Service Rating by Top Airlines')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.  
I use this chart to compares ground service quality among airlines.

##### 2. What is/are the insight(s) found from the chart?

Answer Here From the chart we can see high rated airline is  (China southern Airlines) , medium are  ( Brithish Airways , Emirates) and other are lower rated

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here

  **Positive Business Impact:** Identifying outliers can guide airlines in best practices and highlight areas needing improvement.

**Negative Growth Insight:** Airlines with consistently low ratings may suffer reputational damage, leading to decreased customer trust and loyalty.

#### Chart - 11
"""

# Chart - 11 visualization code
# Entertainment Rating by Cabin Type
sns.violinplot(x='cabin', y='entertainment', data=df)
plt.xlabel('Cabin Type')
plt.ylabel('Entertainment Rating')
plt.title('Entertainment Rating Distribution by Cabin Type')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.  I use this chart shows the distribution of entertainment ratings, revealing customer sentiment regarding in-flight entertainment.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here  

 **Positive Business Impact:** Enhancing entertainment options can be a key differentiator, particularly in competitive markets.


**Negative Growth Insight:** Low ratings may suggest that investments in entertainment are not meeting customer expectations, impacting overall satisfaction.

#### Chart - 12
"""

# Chart - 12 visualization code
# Average Food & Beverage Rating by Airline
food_bev_ratings = df.groupby('airline')['food_bev'].mean().sort_values(ascending=False).head(10)
sns.barplot(x=food_bev_ratings.values, y=food_bev_ratings.index)
plt.xlabel('Average Food & Beverage Rating')
plt.ylabel('Airline')
plt.title('Top 10 Airlines by Food & Beverage Rating')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here. I use this chart to  compares food and beverage quality across airlines.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here  
**Positive Business Impact:** Airlines can use this insight to highlight superior dining experiences in their marketing and improve services where needed.

**Negative Growth Insight:** Low ratings in this area may indicate neglect in catering services, which can affect customer loyalty and retention.

#### Chart - 13
"""

# Chart - 13 visualization code
# Traveller Type by Cabin Class
traveller_cabin_counts = df.groupby(['cabin', 'traveller_type']).size().unstack().fillna(0)
traveller_cabin_counts.plot(kind='bar', stacked=True)
plt.xlabel('Cabin Type')
plt.ylabel('Count')
plt.title('Traveller Type by Cabin Class')
plt.legend(title='Traveller Type')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here. I use this chart illustrates the distribution of traveler types within each cabin class.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

##### 3. Will the gained insights help creating a positive business impact?
Are there any insights that lead to negative growth? Justify with specific reason.

Answer Here  

**Positive Business Impact:** Understanding customer demographics within each cabin can inform targeted marketing and service offerings.

Negative Growth Insight:** An imbalance in traveler types per cabin may indicate misalignment between offerings and customer preferences, necessitating adjustments to improve occupancy and satisfaction.

#### Chart - 14 - Correlation Heatmap
"""

# Correlation Heatmap visualization code
# select numeric data type
numeric_columns = df.select_dtypes(include=['int64' , 'float64'])
corr_matrix = numeric_columns.corr()
sns.heatmap(corr_matrix, annot = True)
plt.title('Correlation Heatmap')
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

#### Chart - 15 - Pair Plot
"""

# Pair Plot visualization code
sns.pairplot(df)
plt.show()

"""##### 1. Why did you pick the specific chart?

Answer Here.

##### 2. What is/are the insight(s) found from the chart?

Answer Here

## ***5. Hypothesis Testing***

### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

Answer Here.

### Hypothetical Statement - 1

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

Answer Here.  
**Null Hypothesis (H0):** There is no significant difference in seat comfort ratings across different cabin types.

**Alternate Hypothesis (H1):** There is a significant difference in seat comfort ratings across different cabin types.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
# Perform ANOVA
f_statistic, p_value = stats.f_oneway(df[df['cabin'] == 'Economy Class']['seat_comfort'],
                                      df[df['cabin'] == 'Business Class']['seat_comfort'],
                                      df[df['cabin'] == 'First Class']['seat_comfort'])
print("F-statistic:", f_statistic)
print("P-value:", p_value)

"""##### Which statistical test have you done to obtain P-Value?

Answer Here.  
I used ANOVA (Analysis of Variance) statistical test to obtain P-value

##### Why did you choose the specific statistical test?

Answer Here.  
ANOVA is suitable here because we are comparing the mean seat comfort ratings across more than two independent groups (cabin types).

### Hypothetical Statement - 2

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

Answer Here.

**Null Hypothesis (H0):** There is no significant difference in overall ratings between different traveler types.

**Alternate Hypothesis (H1):** There is a significant difference in overall ratings between different traveler types.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
business_ratings = df[df['traveller_type'] == 'Business']['overall']
leisure_ratings = df[df['traveller_type'] == 'Family Leisure']['overall']

# Perform independent samples t-test
t_statistic, p_value = stats.ttest_ind(business_ratings, leisure_ratings, equal_var=False)
print("T-statistic:", t_statistic)
print("P-value:", p_value)

"""##### Which statistical test have you done to obtain P-Value?

Answer Here.  
 I used Independent Samples t-test

##### Why did you choose the specific statistical test?

Answer Here.  
I choose this statistical test because  it compares the means of two independent groups (Business and Leisure travelers) for a continuous outcome variable (overall rating).

### Hypothetical Statement - 3

#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

Answer Here.n

**Null Hypothesis (H0):** There is no significant association between ground service ratings and value-for-money ratings.

**Alternate Hypothesis (H1):** There is a significant association between ground service ratings and value-for-money ratings.

#### 2. Perform an appropriate statistical test.
"""

# Perform Statistical Test to obtain P-Value
import scipy.stats as stats

# Calculate Spearman's rank correlation
correlation_coefficient, p_value = stats.spearmanr(df['ground_service'], df['value_for_money'])
print("Spearman Correlation Coefficient:", correlation_coefficient)
print("P-value:", p_value)

"""##### Which statistical test have you done to obtain P-Value?

Answer Here.  i used Spearman’s Rank Correlation

##### Why did you choose the specific statistical test?

Answer Here.

Since both ground service and value-for-money ratings are ordinal variables (likely not normally distributed), Spearman’s rank correlation is suitable for assessing the strength and direction of the association between them.

## ***6. Feature Engineering & Data Pre-processing***

### 1. Handling Missing Values
"""

# Handling Missing Values & Missing Value Imputation

"""#### What all missing value imputation techniques have you used and why did you use those techniques?

Answer Here.

### 2. Handling Outliers
"""

# Handling Outliers & Outlier treatments
# Removing outliers with Z-score method
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
z_scores = np.abs(stats.zscore(df[numerical_cols]))
df = df[(z_scores < 3).all(axis=1)]

"""##### What all outlier treatment techniques have you used and why did you use those techniques?

Answer Here.  I used Z-score technique to handle outliers

### 3. Categorical Encoding
"""

# Encode your categorical columns
# For binary categorical variables, use Label Encoding
label_encoder = LabelEncoder()
df['recommended'] = label_encoder.fit_transform(df['recommended'])
categorical_cols = df.select_dtypes(include=['object']).columns
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
df_encoded.head()

"""#### What all categorical encoding techniques have you used & why did you use those techniques?

Answer Here.
I use label encoding  for column ['recomended']
and one-hot encoding  for all remaining categorical columns

### 4. Textual Data Preprocessing
(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)

#### 1. Expand Contraction
"""

# Expand Contraction
# Contraction dictionary
contractions = {
    "can't": "cannot",
    "won't": "will not",
    "i'm": "i am",
    "he's": "he is",
    "she's": "she is",
    "it's": "it is",
    "we're": "we are",
    "they're": "they are",
    "you're": "you are",
    "i've": "i have",
    "they've": "they have",
    "we've": "we have",
    # add more contractions as needed
}

# Expand contractions function
# def expand_contractions(text):
#     for contraction, expansion in contractions.items():
#         text = re.sub(r'\b' + contraction + r'\b', expansion, text)
#     return text

# Function to expand contractions
def expand_contractions(text):
    for word in text.split():
        if word.lower() in contractions:
            text = text.replace(word, contractions[word.lower()])
    return text

# Apply to column
df['expanded_review'] = df['customer_review'].apply(expand_contractions)

"""#### 2. Lower Casing"""

# Lower Casing
# Lowercase all text
df['lowercase_review'] = df['expanded_review'].str.lower()

"""#### 3. Removing Punctuations"""

# Remove Punctuations
df['remov_punctuation_review'] = df['lowercase_review'].str.replace(r'[^\w\s]', '', regex=True)

"""#### 4. Removing URLs & Removing words and digits contain digits."""

# Remove URLs & Remove words and digits contain digits
df['no_url_review'] = df['remov_punctuation_review'].apply(lambda x: ''.join([word for word in x if not bool(re.search(r'\d', word))]))

"""#### 5. Removing Stopwords & Removing White spaces"""

# Remove Stopwords
# Remove stopwords and extra whitespace
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

df['no_stopwords'] = df['no_url_review'].apply(remove_stopwords)

# Remove White spaces
df['no_whiteapace'] = df['no_stopwords'].apply(lambda x: re.sub(' +', ' ', x).strip())

"""#### 6. Rephrase Text"""

# Rephrase Text
# Apply lemmatization to rephrase words to base form
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words]
    return ' '.join(words)

df['rephrased_review'] = df['no_stopwords'].apply(lemmatize_text)

"""#### 7. Tokenization"""

# Tokenization
df['tocknized_review'] = df['rephrased_review'].apply(word_tokenize)

"""#### 8. Text Normalization"""

# Normalizing Text (i.e., Stemming, Lemmatization etc.)
lemmatizer = WordNetLemmatizer()
df['normalized_text'] = df['tocknized_review'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

"""##### Which text normalization technique have you used and why?

Answer Here.

#### 9. Part of speech tagging
"""

# POS Taging
# POS tagging
df['pos_tags'] = df['normalized_text'].apply(pos_tag)

"""#### 10. Text Vectorization"""



"""##### Which text vectorization technique have you used and why?

Answer Here.

### 4. Feature Manipulation & Selection

#### 1. Feature Manipulation
"""

# Binning overall rating into categories
df['overall_category'] = pd.cut(df['overall'], bins=[0, 3, 6, 10], labels=['Low', 'Medium', 'High'])
# Adding a feature for review length
df['review_length'] = df['customer_review'].apply(lambda x: len(x.split()))
# Encoding 'traveller_type' and 'cabin' using one-hot encoding
df = pd.get_dummies(df, columns=['traveller_type', 'cabin'], drop_first=True)

"""#### 2. Feature Selection"""

# Drop less informative columns based on domain knowledge or low variance
# Example: Dropping 'author', 'review_date', 'route', assuming these have less predictive power
df = df.drop(columns=['author', 'review_date', 'route'])

"""##### What all feature selection methods have you used  and why?

Answer Here.

##### Which all features you found important and why?

Answer Here.

### 5. Data Transformation

#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?
"""

# Transform Your data
# Apply log transformation to skewed numerical columns
for col in numerical_cols:
    if df[col].skew() > 1:
        df[col] = np.log1p(df[col])

"""### 6. Data Scaling"""

# Scaling your data
# Scale numerical features
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

"""##### Which method have you used to scale you data and why?

### 7. Dimesionality Reduction

##### Do you think that dimensionality reduction is needed? Explain Why?

Answer Here.
"""



"""##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)

Answer Here.

### 8. Data Splitting
"""

# Split your data to train and test. Choose Splitting ratio wisely.
# Define target and features
X = df.drop('overall', axis=1)  # Assuming 'overall' is the target variable
y = df['overall']

# Train-Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""##### What data splitting ratio have you used and why?

Answer Here.

### 9. Handling Imbalanced Dataset

##### Do you think the dataset is imbalanced? Explain Why.

Answer Here.
"""



"""##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)

Answer Here.

## ***7. ML Model Implementation***

### ML Model - 1
"""

# Assuming 'df' is the preprocessed dataset and 'recommended' is the target column
X = df.drop(columns=['pos_tags'])
y = df['pos_tags']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."""

from sklearn.preprocessing import MultiLabelBinarizer

# Initialize MultiLabelBinarizer
mlb = MultiLabelBinarizer()

# Apply to target variable 'y' if it's in a multi-label format
y_train_binarized = mlb.fit_transform(y_train)
y_test_binarized = mlb.transform(y_test)

# Use the binarized labels in model training
rf_cv.fit(X_train, y_train_binarized)
y_pred_rf = rf_cv.predict(X_test)

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
# Hyperparameter tuning for Logistic Regression
log_reg = LogisticRegression(solver='liblinear')
log_reg_params = {'C': [0.01, 0.1, 1, 10]}
log_reg_cv = GridSearchCV(log_reg, log_reg_params, cv=5)


# Fit the Algorithm
log_reg_cv.fit(X_train, y_train)

# Predict on the model
y_pred_log = log_reg_cv.predict(X_test)

# Visualizing evaluation Metric Score chart
accuracy_log = accuracy_score(y_test, y_pred_log)
precision_log = precision_score(y_test, y_pred_log)
recall_log = recall_score(y_test, y_pred_log)
f1_log = f1_score(y_test, y_pred_log)

print(f"Logistic Regression - Best Parameters: {log_reg_cv.best_params_}")
print(f"Accuracy: {accuracy_log}, Precision: {precision_log}, Recall: {recall_log}, F1: {f1_log}")

"""##### Which hyperparameter optimization technique have you used and why?

Answer Here.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Answer Here.

### ML Model - 2

1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.
"""

# Visualizing evaluation Metric Score chart
# Create a DataFrame for metrics
metrics_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
    'Score': [accuracy_rf, precision_rf, recall_rf, f1_rf]
})

# Plotting
plt.figure(figsize=(8, 5))
sns.barplot(x='Metric', y='Score', data=metrics_df)
plt.title("Random Forest Model Performance Metrics")
plt.ylim(0, 1)
plt.show()

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)

# Fit the Algorithm

# Predict on the model
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Define the Random Forest model and parameters for tuning
rf = RandomForestClassifier(random_state=42)
rf_params = {
    'n_estimators': [50, 100, 200],     # Number of trees in the forest
    'max_depth': [None, 10, 20],        # Maximum depth of each tree
    'min_samples_split': [2, 5, 10]     # Minimum samples needed to split a node
}

# Cross-validation with GridSearchCV
rf_cv = GridSearchCV(rf, rf_params, cv=5, scoring='accuracy')
rf_cv.fit(X_train, y_train)

# Make predictions using the best model
y_pred_rf = rf_cv.predict(X_test)

# Evaluate performance metrics
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print(f"Random Forest - Best Parameters: {rf_cv.best_params_}")
print(f"Accuracy: {accuracy_rf}, Precision: {precision_rf}, Recall: {recall_rf}, F1: {f1_rf}")

"""##### Which hyperparameter optimization technique have you used and why?

Answer Here.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Answer Here.

#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used.

Answer Here.

### ML Model - 3

#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.
"""

# Visualizing evaluation Metric Score chart

"""#### 2. Cross- Validation & Hyperparameter Tuning"""

# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)
gb = GradientBoostingClassifier(random_state=42)
gb_params = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]}
gb_cv = GridSearchCV(gb, gb_params, cv=5)
# Fit the Algorithm
gb_cv.fit(X_train, y_train)

# Predict on the model
y_pred_gb = gb_cv.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print(f"Random Forest - Best Parameters: {rf_cv.best_params_}")
print(f"Accuracy: {accuracy_rf}, Precision: {precision_rf}, Recall: {recall_rf}, F1: {f1_rf}")

"""##### Which hyperparameter optimization technique have you used and why?

Answer Here.

##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.

Answer Here.

### 1. Which Evaluation metrics did you consider for a positive business impact and why?

Answer Here.

### 2. Which ML model did you choose from the above created models as your final prediction model and why?

Answer Here.

### 3. Explain the model which you have used and the feature importance using any model explainability tool?

Answer Here.

## ***8.*** ***Future Work (Optional)***

### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.
"""

# Save the File

"""### 2. Again Load the saved model file and try to predict unseen data for a sanity check.

"""

# Load the File and predict unseen data.

"""### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***

# **Conclusion**

This project successfully analyzed and modeled customer review data, focusing on insights from various review attributes, traveler types, and cabin classes. The workflow comprised exploratory data analysis (EDA), visualization, feature engineering, and machine learning model creation, providing both descriptive and predictive insights.

### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***
"""